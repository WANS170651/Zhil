"""
ä¸»å¤„ç†æµç¨‹æ¨¡å—
æ•´åˆWebScraperã€Extractorã€Normalizerã€NotionWriterç­‰ç»„ä»¶
å®ç°å®Œæ•´çš„URLâ†’Notionçš„æ•°æ®å¤„ç†ç®¡é“
æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§æ¨¡å¼ï¼Œå¼‚æ­¥æ¨¡å¼æ”¯æŒçœŸæ­£çš„å¹¶å‘å¤„ç†
"""

import asyncio
import time
import logging
import sys
import os
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path

# å¯¼å…¥æ‰€æœ‰å¿…è¦çš„æ¨¡å—
from .web_scraper import WebScraper
from .notion_schema import get_database_schema, get_database_schema_async, DatabaseSchema
from .extractor import extractor, async_extractor, ExtractionMode
from .normalizer import normalizer
from .notion_writer import notion_writer, async_notion_writer, WriteOperation, WriteResult
from .config import config


class ProcessingStage(Enum):
    """å¤„ç†é˜¶æ®µæšä¸¾"""
    VALIDATION = "validation"
    SCRAPING = "scraping"
    EXTRACTION = "extraction"
    NORMALIZATION = "normalization"
    WRITING = "writing"
    COMPLETED = "completed"
    FAILED = "failed"


class ProcessingStatus(Enum):
    """å¤„ç†çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class ProcessingResult:
    """å•ä¸ªURLçš„å¤„ç†ç»“æœ"""
    url: str
    stage: ProcessingStage = ProcessingStage.VALIDATION
    status: ProcessingStatus = ProcessingStatus.PENDING
    
    # å„é˜¶æ®µçš„è¯¦ç»†ç»“æœ
    scraping_result: Optional[str] = None
    extraction_result: Optional[Dict[str, Any]] = None
    normalization_result: Optional[Dict[str, Any]] = None
    writing_result: Optional[WriteResult] = None
    
    # æ—¶é—´ç»Ÿè®¡
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    stage_times: Dict[str, float] = field(default_factory=dict)
    
    # é”™è¯¯ä¿¡æ¯
    error_message: Optional[str] = None
    error_stage: Optional[ProcessingStage] = None
    
    def __post_init__(self):
        if self.start_time is None:
            self.start_time = time.time()
    
    @property
    def total_time(self) -> float:
        """æ€»å¤„ç†æ—¶é—´"""
        if self.end_time and self.start_time:
            return self.end_time - self.start_time
        return 0.0
    
    @property
    def success(self) -> bool:
        """æ˜¯å¦å¤„ç†æˆåŠŸ"""
        return self.status == ProcessingStatus.SUCCESS and self.stage == ProcessingStage.COMPLETED
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        result = {
            "url": self.url,
            "stage": self.stage.value,
            "status": self.status.value,
            "success": self.success,
            "total_time": self.total_time,
            "stage_times": self.stage_times
        }
        
        if self.writing_result:
            result["writing_result"] = self.writing_result.to_dict()
        
        if self.error_message:
            result["error_message"] = self.error_message
            result["error_stage"] = self.error_stage.value if self.error_stage else None
        
        return result


class MainPipelineError(Exception):
    """ä¸»æµç¨‹å¼‚å¸¸"""
    pass


class MainPipeline:
    """ä¸»å¤„ç†æµç¨‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, 
                 headless_browser: bool = True,
                 wait_time: int = 2,
                 max_retries: int = 3,
                 batch_delay: float = 1.0):
        """
        åˆå§‹åŒ–ä¸»æµç¨‹
        
        Args:
            headless_browser: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨
            wait_time: é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            batch_delay: æ‰¹é‡å¤„ç†é—´éš”æ—¶é—´
        """
        self.headless_browser = headless_browser
        self.wait_time = wait_time
        self.max_retries = max_retries
        self.batch_delay = batch_delay
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.web_scraper = WebScraper(headless=headless_browser)
        self.database_schema: Optional[DatabaseSchema] = None
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)


class AsyncMainPipeline:
    """å¼‚æ­¥ä¸»å¤„ç†æµç¨‹ï¼ˆæ”¯æŒçœŸæ­£å¹¶å‘ï¼‰"""
    
    def __init__(self, 
                 headless_browser: bool = True,
                 wait_time: int = 2,
                 max_retries: int = 3,
                 batch_delay: float = 1.0,
                 max_concurrent: int = 5):
        """
        åˆå§‹åŒ–å¼‚æ­¥ä¸»æµç¨‹
        
        Args:
            headless_browser: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨
            wait_time: é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            batch_delay: æ‰¹é‡å¤„ç†é—´éš”æ—¶é—´ï¼ˆå¼‚æ­¥æ¨¡å¼ä¸‹ä¸»è¦ç”¨äºé€Ÿç‡é™åˆ¶ï¼‰
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œé˜²æ­¢è¿‡åº¦å¹¶å‘å¯¼è‡´APIé™æµ
        """
        self.headless_browser = headless_browser
        self.wait_time = wait_time
        self.max_retries = max_retries
        self.batch_delay = batch_delay
        self.max_concurrent = max_concurrent
        
        # å¹¶å‘æ§åˆ¶ä¿¡å·é‡
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # åˆå§‹åŒ–å¼‚æ­¥ç»„ä»¶
        self.web_scraper = WebScraper(headless=headless_browser)
        self.database_schema: Optional[DatabaseSchema] = None
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    async def _load_database_schema_async(self) -> bool:
        """å¼‚æ­¥åŠ è½½æ•°æ®åº“Schema"""
        try:
            if not self.database_schema:
                self.logger.info("ğŸ“‹ å¼‚æ­¥åŠ è½½Notionæ•°æ®åº“Schema...")
                self.database_schema = await get_database_schema_async()
                
                if self.database_schema:
                    self.logger.info(f"âœ… Schemaå¼‚æ­¥åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.database_schema.fields)} ä¸ªå­—æ®µ")
                    return True
                else:
                    self.logger.error("âŒ Schemaå¼‚æ­¥åŠ è½½å¤±è´¥")
                    return False
            return True
        except Exception as e:
            self.logger.error(f"âŒ Schemaå¼‚æ­¥åŠ è½½å¼‚å¸¸: {e}")
            return False
    
    def _validate_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼ï¼ˆä¸åŒæ­¥ç‰ˆæœ¬å…±äº«ï¼‰"""
        try:
            if not url or not isinstance(url, str):
                return False
            
            url = url.strip()
            if not url.startswith(('http://', 'https://')):
                return False
            
            return True
        except Exception:
            return False
    
    async def _extract_information_async(self, scraped_content: str, url: str, result: ProcessingResult) -> bool:
        """å¼‚æ­¥LLMä¿¡æ¯æå–"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.EXTRACTION
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ§  å¼€å§‹å¼‚æ­¥LLMä¿¡æ¯æå–...")
            
            # å¼‚æ­¥LLMè°ƒç”¨
            extraction_result = await async_extractor.extract_async(
                content=scraped_content,
                url=url,
                mode=ExtractionMode.FUNCTION_CALL,
                max_retries=self.max_retries
            )
            
            if extraction_result.success:
                result.extraction_result = extraction_result.to_dict()
                result.status = ProcessingStatus.SUCCESS
                result.stage_times["extraction"] = time.time() - stage_start
                
                extracted_data = extraction_result.data or {}
                self.logger.info(f"âœ… å¼‚æ­¥ä¿¡æ¯æå–æˆåŠŸï¼Œæå– {len(extracted_data)} ä¸ªå­—æ®µ")
                
                # è®°å½•æå–çš„å…³é”®ä¿¡æ¯
                if "Company" in extracted_data:
                    self.logger.info(f"   ğŸ“ å…¬å¸: {extracted_data.get('Company', 'æœªçŸ¥')}")
                if "Position" in extracted_data:
                    self.logger.info(f"   ğŸ’¼ èŒä½: {extracted_data.get('Position', 'æœªçŸ¥')}")
                
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = f"å¼‚æ­¥LLMæå–å¤±è´¥: {extraction_result.error or 'æœªçŸ¥é”™è¯¯'}"
                result.error_stage = ProcessingStage.EXTRACTION
                
                self.logger.error(f"âŒ å¼‚æ­¥LLMä¿¡æ¯æå–å¤±è´¥: {result.error_message}")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"å¼‚æ­¥LLMæå–å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.EXTRACTION
            result.stage_times["extraction"] = time.time() - stage_start
            
            self.logger.error(f"âŒ å¼‚æ­¥LLMæå–å¼‚å¸¸: {e}")
            return False
    
    def _normalize_data(self, extracted_data: Dict[str, Any], result: ProcessingResult) -> bool:
        """æ•°æ®å½’ä¸€åŒ–å’ŒéªŒè¯ï¼ˆä¸åŒæ­¥ç‰ˆæœ¬å…±äº«ï¼ŒCPUå¯†é›†å‹ä¿æŒåŒæ­¥ï¼‰"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.NORMALIZATION
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ”§ å¼€å§‹æ•°æ®å½’ä¸€åŒ–...")
            
            normalization_result = normalizer.normalize(
                raw_data=extracted_data,
                database_schema=self.database_schema
            )
            
            if normalization_result.success:
                result.normalization_result = normalization_result.to_dict()
                result.status = ProcessingStatus.SUCCESS
                result.stage_times["normalization"] = time.time() - stage_start
                
                self.logger.info(f"âœ… æ•°æ®å½’ä¸€åŒ–æˆåŠŸ")
                
                # è®°å½•å½’ä¸€åŒ–ç»Ÿè®¡
                if normalization_result.error_count > 0:
                    self.logger.warning(f"   âš ï¸ å‘ç° {normalization_result.error_count} ä¸ªé”™è¯¯")
                if normalization_result.warning_count > 0:
                    self.logger.info(f"   ğŸ’¡ å‘ç° {normalization_result.warning_count} ä¸ªè­¦å‘Š")
                
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = "æ•°æ®å½’ä¸€åŒ–å¤±è´¥"
                result.error_stage = ProcessingStage.NORMALIZATION
                
                self.logger.error(f"âŒ æ•°æ®å½’ä¸€åŒ–å¤±è´¥")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"æ•°æ®å½’ä¸€åŒ–å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.NORMALIZATION
            result.stage_times["normalization"] = time.time() - stage_start
            
            self.logger.error(f"âŒ æ•°æ®å½’ä¸€åŒ–å¼‚å¸¸: {e}")
            return False
    
    async def _write_to_notion_async(self, normalized_data: Dict[str, Any], result: ProcessingResult) -> bool:
        """å¼‚æ­¥å†™å…¥Notionæ•°æ®åº“"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.WRITING
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ’¾ å¼€å§‹å¼‚æ­¥å†™å…¥Notionæ•°æ®åº“...")
            
            # è·å–å½’ä¸€åŒ–åçš„Notionå±æ€§
            notion_properties = normalized_data.get("notion_payload", {})
            
            # å¼‚æ­¥å†™å…¥
            write_result = await async_notion_writer.upsert_async(
                properties=notion_properties,
                force_create=False  # é»˜è®¤å¯ç”¨æ™ºèƒ½å»é‡
            )
            
            if write_result.success:
                result.writing_result = write_result
                result.status = ProcessingStatus.SUCCESS
                result.stage = ProcessingStage.COMPLETED
                result.stage_times["writing"] = time.time() - stage_start
                result.end_time = time.time()
                
                operation_desc = "åˆ›å»º" if write_result.operation == WriteOperation.CREATE else "æ›´æ–°"
                self.logger.info(f"âœ… å¼‚æ­¥Notionå†™å…¥æˆåŠŸï¼Œ{operation_desc}é¡µé¢: {write_result.page_id}")
                
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = f"å¼‚æ­¥Notionå†™å…¥å¤±è´¥: {write_result.error_message}"
                result.error_stage = ProcessingStage.WRITING
                
                self.logger.error(f"âŒ å¼‚æ­¥Notionå†™å…¥å¤±è´¥: {write_result.error_message}")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"å¼‚æ­¥Notionå†™å…¥å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.WRITING
            result.stage_times["writing"] = time.time() - stage_start
            
            self.logger.error(f"âŒ å¼‚æ­¥Notionå†™å…¥å¼‚å¸¸: {e}")
            return False
    
    async def process_single_url_with_semaphore(self, url: str) -> ProcessingResult:
        """å¸¦å¹¶å‘æ§åˆ¶çš„å•URLå¤„ç†"""
        async with self.semaphore:  # å¹¶å‘æ§åˆ¶
            return await self.process_single_url_async(url)
    
    async def process_single_url_async(self, url: str) -> ProcessingResult:
        """å¼‚æ­¥å¤„ç†å•ä¸ªURL"""
        result = ProcessingResult(url=url)
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥å¤„ç†URL: {url}")
            
            # 1. éªŒè¯URL
            result.stage = ProcessingStage.VALIDATION
            result.status = ProcessingStatus.IN_PROGRESS
            
            if not self._validate_url(url):
                result.status = ProcessingStatus.FAILED
                result.error_message = "URLæ ¼å¼æ— æ•ˆ"
                result.error_stage = ProcessingStage.VALIDATION
                return result
            
            # 2. å¼‚æ­¥åŠ è½½æ•°æ®åº“Schema
            if not await self._load_database_schema_async():
                result.status = ProcessingStatus.FAILED
                result.error_message = "æ•°æ®åº“Schemaå¼‚æ­¥åŠ è½½å¤±è´¥"
                result.error_stage = ProcessingStage.VALIDATION
                return result
            
            # 3. çˆ¬å–é¡µé¢ï¼ˆå·²ç»æ˜¯å¼‚æ­¥ï¼‰
            if not await self._scrape_page_async(url, result):
                return result
            
            # 4. å¼‚æ­¥LLMä¿¡æ¯æå–
            if not await self._extract_information_async(result.scraping_result, url, result):
                return result
            
            # 5. æ•°æ®å½’ä¸€åŒ–ï¼ˆCPUå¯†é›†å‹ï¼Œä¿æŒåŒæ­¥ï¼‰
            extracted_data = result.extraction_result.get("data", {}) if result.extraction_result else {}
            if not self._normalize_data(extracted_data, result):
                return result
            
            # 6. å¼‚æ­¥å†™å…¥Notion
            if not await self._write_to_notion_async(result.normalization_result, result):
                return result
            
            self.logger.info(f"ğŸ‰ å¼‚æ­¥URLå¤„ç†å®Œæˆ: {url} (è€—æ—¶: {result.total_time:.2f}s)")
            
            return result
            
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"å¼‚æ­¥å¤„ç†å¼‚å¸¸: {e}"
            result.end_time = time.time()
            
            self.logger.error(f"âŒ å¼‚æ­¥URLå¤„ç†å¼‚å¸¸: {url} - {e}")
            return result
    
    async def _scrape_page_async(self, url: str, result: ProcessingResult) -> bool:
        """å¼‚æ­¥çˆ¬å–é¡µé¢å†…å®¹"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.SCRAPING
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ•·ï¸ å¼€å§‹å¼‚æ­¥çˆ¬å–é¡µé¢: {url}")
            
            scraped_content = await self.web_scraper.scrape_to_markdown(url, wait_time=self.wait_time)
            
            if scraped_content:
                result.scraping_result = scraped_content
                result.status = ProcessingStatus.SUCCESS
                result.stage_times["scraping"] = time.time() - stage_start
                
                self.logger.info(f"âœ… å¼‚æ­¥é¡µé¢çˆ¬å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(scraped_content)} å­—ç¬¦")
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = "å¼‚æ­¥é¡µé¢çˆ¬å–è¿”å›ç©ºå†…å®¹"
                result.error_stage = ProcessingStage.SCRAPING
                
                self.logger.error(f"âŒ å¼‚æ­¥é¡µé¢çˆ¬å–å¤±è´¥: è¿”å›ç©ºå†…å®¹")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"å¼‚æ­¥é¡µé¢çˆ¬å–å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.SCRAPING
            result.stage_times["scraping"] = time.time() - stage_start
            
            self.logger.error(f"âŒ å¼‚æ­¥é¡µé¢çˆ¬å–å¼‚å¸¸: {e}")
            return False
    
    async def process_multiple_urls_concurrent(self, urls: List[str]) -> List[ProcessingResult]:
        """
        çœŸæ­£çš„å¹¶å‘æ‰¹é‡å¤„ç†ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰
        """
        if not urls:
            self.logger.warning("âš ï¸ URLåˆ—è¡¨ä¸ºç©º")
            return []
        
        self.logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(urls)} ä¸ªURLï¼Œæœ€å¤§å¹¶å‘æ•°: {self.max_concurrent}")
        
        start_time = time.time()
        
        # ä½¿ç”¨asyncio.gatherå®ç°çœŸæ­£å¹¶å‘
        tasks = [self.process_single_url_with_semaphore(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # å¦‚æœæŸä¸ªä»»åŠ¡å‡ºç°å¼‚å¸¸ï¼Œåˆ›å»ºå¤±è´¥ç»“æœ
                error_result = ProcessingResult(url=urls[i])
                error_result.status = ProcessingStatus.FAILED
                error_result.error_message = f"å¹¶å‘å¤„ç†å¼‚å¸¸: {result}"
                error_result.end_time = time.time()
                processed_results.append(error_result)
            else:
                processed_results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        total_time = time.time() - start_time
        success_count = sum(1 for r in processed_results if r.success)
        
        # è®¡ç®—æ€§èƒ½æå‡
        estimated_sequential_time = len(urls) * 15  # å‡è®¾é¡ºåºå¤„ç†æ¯ä¸ª15ç§’
        speedup_ratio = estimated_sequential_time / total_time if total_time > 0 else 1
        
        self.logger.info(f"ğŸ“Š å¹¶å‘æ‰¹é‡å¤„ç†å®Œæˆ!")
        self.logger.info(f"   æ€»æ•°: {len(processed_results)}")
        self.logger.info(f"   æˆåŠŸ: {success_count}")
        self.logger.info(f"   å¤±è´¥: {len(processed_results) - success_count}")
        self.logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        self.logger.info(f"   å¹³å‡è€—æ—¶: {total_time/len(processed_results):.2f}s/ä¸ª")
        self.logger.info(f"   ğŸš€ å¹¶å‘åŠ é€Ÿæ¯”: {speedup_ratio:.1f}x")
        self.logger.info(f"   æˆåŠŸç‡: {success_count/len(processed_results)*100:.1f}%")
        
        return processed_results
    
    def generate_report(self, results: List[ProcessingResult]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤„ç†æŠ¥å‘Šï¼ˆä¸åŒæ­¥ç‰ˆæœ¬å…±äº«é€»è¾‘ï¼‰
        
        Args:
            results: å¤„ç†ç»“æœåˆ—è¡¨
            
        Returns:
            Dict: æŠ¥å‘Šæ•°æ®
        """
        if not results:
            return {"error": "æ²¡æœ‰å¤„ç†ç»“æœ"}
        
        # åŸºæœ¬ç»Ÿè®¡
        total_count = len(results)
        success_count = sum(1 for r in results if r.success)
        failed_count = total_count - success_count
        
        # æ“ä½œç»Ÿè®¡
        create_count = sum(1 for r in results 
                          if r.writing_result and r.writing_result.operation == WriteOperation.CREATE)
        update_count = sum(1 for r in results 
                          if r.writing_result and r.writing_result.operation == WriteOperation.UPDATE)
        
        # æ—¶é—´ç»Ÿè®¡
        total_time = sum(r.total_time for r in results)
        avg_time = total_time / total_count if total_count > 0 else 0
        
        # é˜¶æ®µè€—æ—¶ç»Ÿè®¡
        stage_times = {}
        for stage in ["scraping", "extraction", "normalization", "writing"]:
            times = [r.stage_times.get(stage, 0) for r in results if r.stage_times.get(stage, 0) > 0]
            if times:
                stage_times[stage] = {
                    "avg": sum(times) / len(times),
                    "max": max(times),
                    "min": min(times)
                }
        
        # é”™è¯¯ç»Ÿè®¡
        error_stages = {}
        for result in results:
            if result.error_stage:
                stage = result.error_stage.value
                error_stages[stage] = error_stages.get(stage, 0) + 1
        
        # è®¡ç®—æ€§èƒ½æå‡ï¼ˆä»…å¯¹å¼‚æ­¥ç‰ˆæœ¬ï¼‰
        estimated_sequential_time = total_count * 15  # å‡è®¾é¡ºåºå¤„ç†æ¯ä¸ª15ç§’
        actual_wall_time = max(r.total_time for r in results) if results else 0
        speedup_ratio = estimated_sequential_time / actual_wall_time if actual_wall_time > 0 else 1
        
        report = {
            "summary": {
                "total_count": total_count,
                "success_count": success_count,
                "failed_count": failed_count,
                "success_rate": success_count / total_count * 100 if total_count > 0 else 0,
                "estimated_speedup": speedup_ratio  # æ–°å¢ï¼šæ€§èƒ½æå‡å€æ•°
            },
            "operations": {
                "create_count": create_count,
                "update_count": update_count
            },
            "timing": {
                "total_time": total_time,
                "average_time": avg_time,
                "wall_clock_time": actual_wall_time,  # æ–°å¢ï¼šå®é™…å¢™é’Ÿæ—¶é—´
                "stage_times": stage_times
            },
            "errors": {
                "error_stages": error_stages,
                "failed_urls": [r.url for r in results if not r.success]
            },
            "details": [r.to_dict() for r in results]
        }
        
        return report

# ç»§ç»­åŒæ­¥ç‰ˆæœ¬çš„_load_database_schemaæ–¹æ³•
    def _load_database_schema(self) -> bool:
        """åŠ è½½æ•°æ®åº“Schema"""
        try:
            if not self.database_schema:
                self.logger.info("ğŸ“‹ åŠ è½½Notionæ•°æ®åº“Schema...")
                self.database_schema = get_database_schema()
                
                if self.database_schema:
                    self.logger.info(f"âœ… SchemaåŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.database_schema.fields)} ä¸ªå­—æ®µ")
                    return True
                else:
                    self.logger.error("âŒ SchemaåŠ è½½å¤±è´¥")
                    return False
            return True
        except Exception as e:
            self.logger.error(f"âŒ SchemaåŠ è½½å¼‚å¸¸: {e}")
            return False
    
    def _validate_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼"""
        try:
            if not url or not isinstance(url, str):
                return False
            
            url = url.strip()
            if not url.startswith(('http://', 'https://')):
                return False
            
            return True
        except Exception:
            return False
    
    async def _scrape_page(self, url: str, result: ProcessingResult) -> bool:
        """çˆ¬å–é¡µé¢å†…å®¹"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.SCRAPING
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–é¡µé¢: {url}")
            
            scraped_content = await self.web_scraper.scrape_to_markdown(url, wait_time=self.wait_time)
            
            if scraped_content:
                result.scraping_result = scraped_content
                result.status = ProcessingStatus.SUCCESS
                result.stage_times["scraping"] = time.time() - stage_start
                
                self.logger.info(f"âœ… é¡µé¢çˆ¬å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(scraped_content)} å­—ç¬¦")
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = "é¡µé¢çˆ¬å–è¿”å›ç©ºå†…å®¹"
                result.error_stage = ProcessingStage.SCRAPING
                
                self.logger.error(f"âŒ é¡µé¢çˆ¬å–å¤±è´¥: è¿”å›ç©ºå†…å®¹")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"é¡µé¢çˆ¬å–å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.SCRAPING
            result.stage_times["scraping"] = time.time() - stage_start
            
            self.logger.error(f"âŒ é¡µé¢çˆ¬å–å¼‚å¸¸: {e}")
            return False
    
    def _extract_information(self, scraped_content: str, url: str, result: ProcessingResult) -> bool:
        """ä½¿ç”¨LLMæå–ä¿¡æ¯"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.EXTRACTION
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ§  å¼€å§‹LLMä¿¡æ¯æå–...")
            
            extraction_result = extractor.extract(
                content=scraped_content,
                url=url,
                mode=ExtractionMode.FUNCTION_CALL,
                max_retries=self.max_retries
            )
            
            if extraction_result.success:
                result.extraction_result = extraction_result.to_dict()
                result.status = ProcessingStatus.SUCCESS
                result.stage_times["extraction"] = time.time() - stage_start
                
                extracted_data = extraction_result.data or {}
                self.logger.info(f"âœ… ä¿¡æ¯æå–æˆåŠŸï¼Œæå– {len(extracted_data)} ä¸ªå­—æ®µ")
                
                # è®°å½•æå–çš„å…³é”®ä¿¡æ¯
                if "Company" in extracted_data:
                    self.logger.info(f"   ğŸ“ å…¬å¸: {extracted_data.get('Company', 'æœªçŸ¥')}")
                if "Position" in extracted_data:
                    self.logger.info(f"   ğŸ’¼ èŒä½: {extracted_data.get('Position', 'æœªçŸ¥')}")
                
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = f"LLMæå–å¤±è´¥: {extraction_result.error or 'æœªçŸ¥é”™è¯¯'}"
                result.error_stage = ProcessingStage.EXTRACTION
                
                self.logger.error(f"âŒ LLMä¿¡æ¯æå–å¤±è´¥: {result.error_message}")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"LLMæå–å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.EXTRACTION
            result.stage_times["extraction"] = time.time() - stage_start
            
            self.logger.error(f"âŒ LLMæå–å¼‚å¸¸: {e}")
            return False
    
    def _normalize_data(self, extracted_data: Dict[str, Any], result: ProcessingResult) -> bool:
        """æ•°æ®å½’ä¸€åŒ–å’ŒéªŒè¯"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.NORMALIZATION
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ”§ å¼€å§‹æ•°æ®å½’ä¸€åŒ–...")
            
            normalization_result = normalizer.normalize(
                raw_data=extracted_data,
                database_schema=self.database_schema
            )
            
            if normalization_result.success:
                result.normalization_result = normalization_result.to_dict()
                result.status = ProcessingStatus.SUCCESS
                result.stage_times["normalization"] = time.time() - stage_start
                
                self.logger.info(f"âœ… æ•°æ®å½’ä¸€åŒ–æˆåŠŸ")
                
                # è®°å½•å½’ä¸€åŒ–ç»Ÿè®¡
                if normalization_result.error_count > 0:
                    self.logger.warning(f"   âš ï¸ å‘ç° {normalization_result.error_count} ä¸ªé”™è¯¯")
                if normalization_result.warning_count > 0:
                    self.logger.info(f"   ğŸ’¡ å‘ç° {normalization_result.warning_count} ä¸ªè­¦å‘Š")
                
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = "æ•°æ®å½’ä¸€åŒ–å¤±è´¥"
                result.error_stage = ProcessingStage.NORMALIZATION
                
                self.logger.error(f"âŒ æ•°æ®å½’ä¸€åŒ–å¤±è´¥")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"æ•°æ®å½’ä¸€åŒ–å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.NORMALIZATION
            result.stage_times["normalization"] = time.time() - stage_start
            
            self.logger.error(f"âŒ æ•°æ®å½’ä¸€åŒ–å¼‚å¸¸: {e}")
            return False
    
    def _write_to_notion(self, normalized_data: Dict[str, Any], result: ProcessingResult) -> bool:
        """å†™å…¥Notionæ•°æ®åº“"""
        stage_start = time.time()
        
        try:
            result.stage = ProcessingStage.WRITING
            result.status = ProcessingStatus.IN_PROGRESS
            
            self.logger.info(f"ğŸ’¾ å¼€å§‹å†™å…¥Notionæ•°æ®åº“...")
            
            # è·å–å½’ä¸€åŒ–åçš„Notionå±æ€§
            notion_properties = normalized_data.get("notion_payload", {})
            
            write_result = notion_writer.upsert(
                properties=notion_properties,
                force_create=False  # é»˜è®¤å¯ç”¨æ™ºèƒ½å»é‡
            )
            
            if write_result.success:
                result.writing_result = write_result
                result.status = ProcessingStatus.SUCCESS
                result.stage = ProcessingStage.COMPLETED
                result.stage_times["writing"] = time.time() - stage_start
                result.end_time = time.time()
                
                operation_desc = "åˆ›å»º" if write_result.operation == WriteOperation.CREATE else "æ›´æ–°"
                self.logger.info(f"âœ… Notionå†™å…¥æˆåŠŸï¼Œ{operation_desc}é¡µé¢: {write_result.page_id}")
                
                return True
            else:
                result.status = ProcessingStatus.FAILED
                result.error_message = f"Notionå†™å…¥å¤±è´¥: {write_result.error_message}"
                result.error_stage = ProcessingStage.WRITING
                
                self.logger.error(f"âŒ Notionå†™å…¥å¤±è´¥: {write_result.error_message}")
                return False
                
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"Notionå†™å…¥å¼‚å¸¸: {e}"
            result.error_stage = ProcessingStage.WRITING
            result.stage_times["writing"] = time.time() - stage_start
            
            self.logger.error(f"âŒ Notionå†™å…¥å¼‚å¸¸: {e}")
            return False
    
    async def process_single_url(self, url: str) -> ProcessingResult:
        """
        å¤„ç†å•ä¸ªURL
        
        Args:
            url: è¦å¤„ç†çš„URL
            
        Returns:
            ProcessingResult: å¤„ç†ç»“æœ
        """
        result = ProcessingResult(url=url)
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†URL: {url}")
            
            # 1. éªŒè¯URL
            result.stage = ProcessingStage.VALIDATION
            result.status = ProcessingStatus.IN_PROGRESS
            
            if not self._validate_url(url):
                result.status = ProcessingStatus.FAILED
                result.error_message = "URLæ ¼å¼æ— æ•ˆ"
                result.error_stage = ProcessingStage.VALIDATION
                return result
            
            # 2. åŠ è½½æ•°æ®åº“Schema
            if not self._load_database_schema():
                result.status = ProcessingStatus.FAILED
                result.error_message = "æ•°æ®åº“SchemaåŠ è½½å¤±è´¥"
                result.error_stage = ProcessingStage.VALIDATION
                return result
            
            # 3. çˆ¬å–é¡µé¢
            if not await self._scrape_page(url, result):
                return result
            
            # 4. LLMä¿¡æ¯æå–
            if not self._extract_information(result.scraping_result, url, result):
                return result
            
            # 5. æ•°æ®å½’ä¸€åŒ–
            extracted_data = result.extraction_result.get("data", {}) if result.extraction_result else {}
            if not self._normalize_data(extracted_data, result):
                return result
            
            # 6. å†™å…¥Notion
            if not self._write_to_notion(result.normalization_result, result):
                return result
            
            self.logger.info(f"ğŸ‰ URLå¤„ç†å®Œæˆ: {url} (è€—æ—¶: {result.total_time:.2f}s)")
            
            return result
            
        except Exception as e:
            result.status = ProcessingStatus.FAILED
            result.error_message = f"å¤„ç†å¼‚å¸¸: {e}"
            result.end_time = time.time()
            
            self.logger.error(f"âŒ URLå¤„ç†å¼‚å¸¸: {url} - {e}")
            return result
    
    async def process_multiple_urls(self, urls: List[str]) -> List[ProcessingResult]:
        """
        æ‰¹é‡å¤„ç†å¤šä¸ªURL
        
        Args:
            urls: URLåˆ—è¡¨
            
        Returns:
            List[ProcessingResult]: å¤„ç†ç»“æœåˆ—è¡¨
        """
        if not urls:
            self.logger.warning("âš ï¸ URLåˆ—è¡¨ä¸ºç©º")
            return []
        
        self.logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(urls)} ä¸ªURL...")
        
        results = []
        start_time = time.time()
        
        for i, url in enumerate(urls):
            self.logger.info(f"ğŸ“‹ å¤„ç†ç¬¬ {i + 1}/{len(urls)} ä¸ªURL...")
            
            result = await self.process_single_url(url)
            results.append(result)
            
            # æ‰¹é‡å¤„ç†é—´éš”
            if i < len(urls) - 1:
                await asyncio.sleep(self.batch_delay)
        
        # ç»Ÿè®¡ç»“æœ
        total_time = time.time() - start_time
        success_count = sum(1 for r in results if r.success)
        
        self.logger.info(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ!")
        self.logger.info(f"   æ€»æ•°: {len(results)}")
        self.logger.info(f"   æˆåŠŸ: {success_count}")
        self.logger.info(f"   å¤±è´¥: {len(results) - success_count}")
        self.logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        self.logger.info(f"   å¹³å‡è€—æ—¶: {total_time/len(results):.2f}s/ä¸ª")
        self.logger.info(f"   æˆåŠŸç‡: {success_count/len(results)*100:.1f}%")
        
        return results
    
    def generate_report(self, results: List[ProcessingResult]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        
        Args:
            results: å¤„ç†ç»“æœåˆ—è¡¨
            
        Returns:
            Dict: æŠ¥å‘Šæ•°æ®
        """
        if not results:
            return {"error": "æ²¡æœ‰å¤„ç†ç»“æœ"}
        
        # åŸºæœ¬ç»Ÿè®¡
        total_count = len(results)
        success_count = sum(1 for r in results if r.success)
        failed_count = total_count - success_count
        
        # æ“ä½œç»Ÿè®¡
        create_count = sum(1 for r in results 
                          if r.writing_result and r.writing_result.operation == WriteOperation.CREATE)
        update_count = sum(1 for r in results 
                          if r.writing_result and r.writing_result.operation == WriteOperation.UPDATE)
        
        # æ—¶é—´ç»Ÿè®¡
        total_time = sum(r.total_time for r in results)
        avg_time = total_time / total_count if total_count > 0 else 0
        
        # é˜¶æ®µè€—æ—¶ç»Ÿè®¡
        stage_times = {}
        for stage in ["scraping", "extraction", "normalization", "writing"]:
            times = [r.stage_times.get(stage, 0) for r in results if r.stage_times.get(stage, 0) > 0]
            if times:
                stage_times[stage] = {
                    "avg": sum(times) / len(times),
                    "max": max(times),
                    "min": min(times)
                }
        
        # é”™è¯¯ç»Ÿè®¡
        error_stages = {}
        for result in results:
            if result.error_stage:
                stage = result.error_stage.value
                error_stages[stage] = error_stages.get(stage, 0) + 1
        
        report = {
            "summary": {
                "total_count": total_count,
                "success_count": success_count,
                "failed_count": failed_count,
                "success_rate": success_count / total_count * 100 if total_count > 0 else 0
            },
            "operations": {
                "create_count": create_count,
                "update_count": update_count
            },
            "timing": {
                "total_time": total_time,
                "average_time": avg_time,
                "stage_times": stage_times
            },
            "errors": {
                "error_stages": error_stages,
                "failed_urls": [r.url for r in results if not r.success]
            },
            "details": [r.to_dict() for r in results]
        }
        
        return report


# å…¨å±€ä¸»æµç¨‹å®ä¾‹
main_pipeline = MainPipeline()
async_main_pipeline = AsyncMainPipeline()


async def process_url(url: str) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šå¤„ç†å•ä¸ªURLï¼ˆåŒæ­¥ç®¡é“çš„å¼‚æ­¥æ¥å£ï¼‰"""
    result = await main_pipeline.process_single_url(url)
    return result.to_dict()


async def process_url_async(url: str) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šå¼‚æ­¥å¤„ç†å•ä¸ªURL"""
    result = await async_main_pipeline.process_single_url_async(url)
    return result.to_dict()


async def process_urls(urls: List[str]) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡å¤„ç†URLï¼ˆåŒæ­¥ç®¡é“çš„å¼‚æ­¥æ¥å£ï¼‰"""
    results = await main_pipeline.process_multiple_urls(urls)
    report = main_pipeline.generate_report(results)
    return report


async def process_urls_concurrent(urls: List[str]) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šå¹¶å‘æ‰¹é‡å¤„ç†URL"""
    results = await async_main_pipeline.process_multiple_urls_concurrent(urls)
    report = async_main_pipeline.generate_report(results)
    return report


def test_pipeline_connection() -> bool:
    """æµ‹è¯•ç®¡é“å„ç»„ä»¶è¿æ¥"""
    try:
        logging.info("ğŸ”— æµ‹è¯•ç®¡é“ç»„ä»¶è¿æ¥...")
        
        # æµ‹è¯•Notionè¿æ¥
        if not notion_writer.test_connection():
            logging.error("âŒ Notionè¿æ¥å¤±è´¥")
            return False
        
        # æµ‹è¯•LLMè¿æ¥
        if not extractor.test_connection():
            logging.error("âŒ LLMè¿æ¥å¤±è´¥")
            return False
        
        # æµ‹è¯•SchemaåŠ è½½
        schema = get_database_schema()
        if not schema:
            logging.error("âŒ SchemaåŠ è½½å¤±è´¥")
            return False
        
        logging.info("âœ… ç®¡é“ç»„ä»¶è¿æ¥æ­£å¸¸")
        return True
        
    except Exception as e:
        logging.error(f"âŒ ç®¡é“è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        return False
